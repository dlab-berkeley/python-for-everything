{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` is a Machine Learning library for Python, in this tutorial we will learn about the `scikit-learn` interface walking through a regression problem, in details:\n",
    "\n",
    "* load a dataset in `.csv` format with `pandas`\n",
    "* preprocess the data to make them suitable for `scikit-learn`\n",
    "* fit a Decision Tree to the data\n",
    "* compare the predicted and true values\n",
    "* plot the learning curve\n",
    "* use cross validation to tune model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install `scikit-learn` with `conda` (recommended) or `pip`:\n",
    "\n",
    "    conda install scikit-learn\n",
    "\n",
    "or:\n",
    "\n",
    "    pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset: predict Abalone age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abalone is a mollusc with a peculiar ear-shaped shell lined of mother of pearl.\n",
    "Its age can be estimated counting the number of rings in their shell with a microscope, but it is a time consuming process, in this tutorial we will use Machine Learning to predict the age using physical measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picture of the shell of an Abalone:\n",
    "![Abalone](http://upload.wikimedia.org/wikipedia/commons/0/0b/AbaloneInside.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available from the [University of California Irvine Machine Learning data repository](http://archive.ics.uci.edu/ml/datasets/Abalone), we can download the [data in Comma-Separated Values format](http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data) in the current folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no column labels in the data, so we copy them from the documentation and use `pandas` to read and print few lines of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\", \n",
    "                \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n",
    "data = pd.read_csv(\"abalone.data\", names=column_names)\n",
    "print(\"Number of samples: %d\" % len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex is either *Male* (\"M\"), *Female* (\"F\") or *Infant* (\"I\"), this is not suitable for regression algorithms, so we create a binary/boolean feature for each of the 3 options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for more complicated cases use sklearn.feature_extraction.DictVectorizer\n",
    "for label in \"MFI\":\n",
    "    data[label] = data[\"sex\"] == label\n",
    "del data[\"sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in `sklearn` are structured as:\n",
    "\n",
    "* a 2D `numpy` array (n_samples, n_features), where each column is a feature (e.g. sex and physical dimensions) for each sample, e.g. each measured Abalone.\n",
    "* a 1D `numpy` array (n_samples) of the value we aim to predict, e.g. the number of rings, standard variable name `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = data.rings.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del data[\"rings\"] # remove rings from data, so we can convert all the dataframe to a numpy 2D array.\n",
    "X = data.values.astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in Training and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split the data into training and validation sets and use Machine Learning to create an estimator that can learn from  the training set and then check its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "train_X, test_X, train_y, test_y = cross_validation.train_test_split(X, y) # splits 75%/25% by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Decision Tree to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simpler method is a Decision Tree, see this example:\n",
    "![decision_tree_example.png](intermediate-fig/decision_tree_example.png)\n",
    "A Decision Tree learns from available data the best conditions to set on the features, e.g. the purpose and the programming language, to predict an output value, e.g. what format should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DecisionTreeRegressor` is a similar algorithm used to estimate a continous variable instead of a discrete one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# create an estimator, optionally specifying parameters\n",
    "model = DecisionTreeRegressor()\n",
    "# fit the estimator to the data\n",
    "model.fit(train_X, train_y)\n",
    "# apply the model to the test and training data\n",
    "predicted_test_y = model.predict(test_X)\n",
    "predicted_train_y = model.predict(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the results with a scatter-plot of the true number of rings against the predicted number of rings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scatter_y(true_y, predicted_y):\n",
    "    \"\"\"Scatter-plot the predicted vs true number of rings\n",
    "    \n",
    "    Plots:\n",
    "       * predicted vs true number of rings\n",
    "       * perfect agreement line\n",
    "       * +2/-2 number dotted lines\n",
    "\n",
    "    Returns the root mean square of the error\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(true_y, predicted_y, '.k')\n",
    "    \n",
    "    ax.plot([0, 30], [0, 30], '--k')\n",
    "    ax.plot([0, 30], [2, 32], ':k')\n",
    "    ax.plot([2, 32], [0, 30], ':k')\n",
    "    \n",
    "    rms = (true_y - predicted_y).std()\n",
    "    \n",
    "    ax.text(25, 3,\n",
    "            \"Root Mean Square Error = %.2g\" % rms,\n",
    "            ha='right', va='bottom')\n",
    "\n",
    "    ax.set_xlim(0, 30)\n",
    "    ax.set_ylim(0, 30)\n",
    "    \n",
    "    ax.set_xlabel('True number of rings')\n",
    "    ax.set_ylabel('Predicted number of rings')\n",
    "    \n",
    "    return rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatter_y(train_y, predicted_train_y)\n",
    "plt.title(\"Training data\")\n",
    "scatter_y(test_y, predicted_test_y)\n",
    "plt.title(\"Test data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree overfits the training set, i.e. its parameters are fine tuned to reproduce the results of the training set but generalized badly to data not seen previously.\n",
    "\n",
    "To prevent this issue we specify a maximum depth of the decision tree of $10$, so that the estimator does not \"specialize\" too much on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(max_depth=10)\n",
    "# fit the estimator to the data\n",
    "model.fit(train_X, train_y)\n",
    "# apply the model to the test and train data\n",
    "predicted_test_y = model.predict(test_X)\n",
    "predicted_train_y = model.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatter_y(train_y, predicted_train_y)\n",
    "plt.title(\"Training data\")\n",
    "rms_decision_tree = scatter_y(test_y, predicted_test_y)\n",
    "plt.title(\"Test data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_percentage_array = np.linspace(10, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_error = []\n",
    "test_error = []\n",
    "for data_percentage in data_percentage_array:\n",
    "    model = DecisionTreeRegressor(max_depth=10)\n",
    "    number_of_samples = int(data_percentage / 100. * len(train_y))\n",
    "    model.fit(train_X[:number_of_samples,:], train_y[:number_of_samples])\n",
    "\n",
    "    predicted_train_y = model.predict(train_X)\n",
    "    predicted_test_y = model.predict(test_X)\n",
    "\n",
    "    train_error.append((predicted_train_y - train_y).std())\n",
    "    test_error.append((predicted_test_y - test_y).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(data_percentage_array, train_error, label='training')\n",
    "plt.plot(data_percentage_array, test_error, label='validation')\n",
    "plt.grid()\n",
    "plt.legend(loc=3)\n",
    "plt.xlabel('Data percentage')\n",
    "plt.ylabel('Root mean square error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the learning curve, we see that the training error decreases as the `DecisionTree` fits better the data, but the validation error shows a significant gap compared to training error.\n",
    "That typically means the model is not fitting the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Random Forest estimator to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it is useful to test a more sophisticated model, for example Random Forests, i.e. a method that trains several Decision Trees and averages them.\n",
    "\n",
    "`sklearn` has a very consistent interface, for any estimator, call `fit` for training then `predict` to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=5)\n",
    "model.fit(train_X, train_y)\n",
    "predicted_test_y = model.predict(test_X)\n",
    "rms_random_forest = scatter_y(test_y, predicted_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Root Mean Square error decreases from %.2g to %.2g.\" % (rms_decision_tree, rms_random_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we have not yet performed any optimization of the Random Forest parameters:\n",
    "\n",
    "  * `max_depth`: the maximum depth of the Decision Trees\n",
    "  * `max_features`: the max number of features to consider for each split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a list (or distributions) of parameters, `RandomizedSearchCV` performs a randomized search (less expensive than an exhaustive grid search) for the best parameters.\n",
    "\n",
    "`RandomizedSearchCV` uses cross validation, i.e. randomly splits the input data in order to estimate the performance of each model more robustly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "grid = RandomizedSearchCV(model, n_iter=20, \n",
    "            param_distributions=dict(\n",
    "                                          max_depth=np.arange(5,20+1), \n",
    "                                          max_features=np.arange(1, n_features+1)\n",
    "                                    )\n",
    "         )\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(max_features=grid.best_params_[\"max_features\"],\n",
    "                              max_depth=grid.best_params_[\"max_depth\"])\n",
    "model.fit(train_X, train_y)\n",
    "predicted_test_y = model.predict(test_X)\n",
    "rms_optimized_random_forest = scatter_y(test_y, predicted_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Root Mean Square error decreases from %.2g to %.2g.\" % (rms_random_forest, rms_optimized_random_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we learnt about:\n",
    "\n",
    "   * Prepare `y` vector of outputs and `X` matrix of features for `sklearn`\n",
    "   * Split train and test sets: `cross_validation.train_test_split(X, y)`\n",
    "   * Train an estimator: `model.fit(train_X, train_y)`\n",
    "   * Use a trained estimator to predict unknown output values: `model.predict(test_X)`\n",
    "   * Use `RandomizedSearchCV` to optimize model parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
